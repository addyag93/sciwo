{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b07f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "ADDITIONAL_STOPWORDS = ['sc','ieee','cvpr','conference','nips','acm','ieee','kdd','1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021']\n",
    "science_stopwords = ['paper','papers','author','propose','result','results','show','experiments','present','novel','approach','include','included','proceeding','proceedings','contained','contains','high level','rights','right','reserved','ieee',\n",
    "                    'method','datset','corpus','copyright','sae','time','average','experimental','experiments','experiment','copyright','sae','datasets','performance','training','test','state','art','_','machine learning','deep learning','abstract','authors','topics','discussed','given','discussed','discuss','proof']\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d3db4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_stopwords = [\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]\n",
    "science_stopwords = science_stopwords+other_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0109d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "093478df",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(science_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5dd797ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    \"\"\"\n",
    "      A simple function to clean up the data. All the words that\n",
    "      are not designated as a stop word is then lemmatized after\n",
    "      encoding and basic regex parsing are performed.\n",
    "    \"\"\"\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    porter_stemmer=PorterStemmer()\n",
    "    stopwords =list(my_stop_words)\n",
    "#     print(stopwords)\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "    .encode('ascii', 'ignore')\n",
    "    .decode('utf-8', 'ignore')\n",
    "    .lower())\n",
    "     # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    final_words = [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
    "#     final_words=[porter_stemmer.stem(word=word) for word in final_words]\n",
    "    text =' '.join(final_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "91301170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(text):\n",
    "    # create a space between special characters \n",
    "    print(\"tokenize\",text)\n",
    "    text=re.sub(\"(\\\\W)\",\" \\\\1 \",text)\n",
    "\n",
    "    # split based on whitespace\n",
    "    return re.split(\"\\\\s+\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0cf27613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# init stemmer\n",
    "porter_stemmer=PorterStemmer()\n",
    "\n",
    "def my_cool_preprocessor(text):\n",
    "    \n",
    "    text=text.lower() \n",
    "    text=re.sub(\"\\\\W\",\" \",text) # remove special chars\n",
    "#     text=re.sub(\"\\\\s+(in|the|all|for|and|on)\\\\s+\",\" _connector_ \",text) # normalize certain words\n",
    "    \n",
    "    # stem words\n",
    "    words=re.split(\"\\\\s+\",text)\n",
    "    stemmed_words=[porter_stemmer.stem(word=word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "942fdd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_clean_nos(essay):\n",
    "    nonPunctEssay = re.sub(\"[^a-zA-Z\\\\s+,.]\", \" \", essay)\n",
    "    nonPunctEssay = re.sub(\"[^a-zA-Z]\", \" \", nonPunctEssay)\n",
    "    return nonPunctEssay\n",
    "def reg_remove_puntuation(each_essay):\n",
    "    each_essay = unicodedata.normalize('NFKD', each_essay).encode('ascii','ignore')\n",
    "    each_essay = each_essay.decode('utf-8')\n",
    "    each_essay= each_essay.replace(\"’\", \"'\")\n",
    "    each_essay= each_essay.replace(\"…\", \" \")\n",
    "    each_essay= each_essay.replace(\".\", \" \")\n",
    "    \n",
    "    each_essay = re.sub('\\[*?\\]\\\"\\'', '', each_essay)\n",
    "    each_essay = re.sub(\"[,?\\[\\]\\{\\}\\(\\):;<>\\\\\\/\\-_]\", \" \", each_essay)\n",
    "    nonPunctEssay = re.sub(\"[,?\\[\\]\\{\\}\\(\\)\\\":;<>]\", \" \", each_essay)\n",
    "    each_essay=each_essay.replace(\"@\", \" at \")\n",
    "    each_essay=each_essay.replace(\"#\", \" hash \")\n",
    "    each_essay=each_essay.replace(\"$\", \" dollar \")\n",
    "    each_essay=each_essay.replace(\"%\", \" percent \")\n",
    "    each_essay=each_essay.replace(\"'l \", \"l \")\n",
    "    nonPunctEssay = re.sub(\"[,?\\[\\]\\{\\}\\(\\)\\\"\\':;<>\\\\\\/\\-_]\", \"\", each_essay)\n",
    "    \n",
    "    return nonPunctEssay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec3142f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =['Authors','Title','Source_Title','Year','Link','Cited','Abstract','AuthorKey','Eng_Controlled','Eng_Unctrolled','Eng_MainHeading','Scival_topic','Scival_Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b167045",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvpr = pd.read_csv('../Data/Paper_Details/PaperDetails_Gr2021_CVPR.csv',header=None)\n",
    "nips =pd.read_csv('../Data/Paper_Details/PaperDetails_Gr2021_NIPS.csv',header=None)\n",
    "kdd =pd.read_csv('../Data/Paper_Details/PaperDetails_Gr2021_KDD.csv',header=None)\n",
    "acm =pd.read_csv('../Data/Paper_Details/PaperDetails_Gr2021_ACM.csv',header=None)\n",
    "ieee =pd.read_csv('../Data/Paper_Details/PaperDetails_Gr2021_IEEE.csv',header=None)\n",
    "ind =pd.read_csv('../Data/Paper_Details/PaperDetails_Gr2021_IND.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a429410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvpr.columns =cols\n",
    "nips.columns =cols\n",
    "kdd.columns =cols\n",
    "acm.columns =cols\n",
    "ieee.columns =cols\n",
    "ind.columns =cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e0b1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "for df in [cvpr,nips,kdd,acm,ieee,ind]:\n",
    "    final_df = final_df.append(df)\n",
    "final_df.reset_index(drop=True,inplace=True)\n",
    "final_df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ca628129",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_gap = 5\n",
    "start_year = 2000\n",
    "end_year = 2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c72dc68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_cols = ['Abstract','AuthorKey','Eng_Controlled','Eng_Unctrolled','Eng_MainHeading','Scival_topic']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ff8914",
   "metadata": {},
   "source": [
    "##  COunt Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "80dbd5bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year range ['2000-2001', '2001-2002', '2002-2003', '2003-2004', '2004-2005', '2005-2006', '2006-2007', '2007-2008', '2008-2009', '2009-2010', '2010-2011', '2011-2012', '2012-2013', '2013-2014', '2014-2015', '2015-2016', '2016-2017', '2017-2018', '2018-2019', '2019-2020', '2020-2021', '2021-2022']\n",
      "1 (1, 1)\n",
      "length of df 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11114/2026202.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  working_df['text'] =''\n",
      "/tmp/ipykernel_11114/2026202.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  working_df['text'] = working_df['text'] + working_df[col]\n",
      "/tmp/ipykernel_11114/2026202.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  working_df['text'] = working_df['text'].apply(lambda x:reg_remove_puntuation(x))\n",
      "/tmp/ipykernel_11114/2026202.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  working_df['text'] = working_df['text'].apply(lambda x:basic_clean(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2001\n",
      "2001 2002\n",
      "2002 2003\n",
      "2003 2004\n",
      "2004 2005\n",
      "2005 2006\n",
      "2006 2007\n",
      "2007 2008\n",
      "2008 2009\n",
      "2009 2010\n",
      "2010 2011\n",
      "2011 2012\n",
      "2012 2013\n",
      "2013 2014\n",
      "2014 2015\n",
      "2015 2016\n",
      "2016 2017\n",
      "2017 2018\n",
      "2018 2019\n",
      "2019 2020\n",
      "2020 2021\n",
      "2021 2022\n",
      "file_write completed...\n",
      "1 (2, 2)\n",
      "length of df 0\n",
      "2000 2001\n",
      "2001 2002\n",
      "2002 2003\n",
      "2003 2004\n",
      "2004 2005\n",
      "2005 2006\n",
      "2006 2007\n",
      "2007 2008\n",
      "2008 2009\n",
      "2009 2010\n",
      "2010 2011\n",
      "2011 2012\n",
      "2012 2013\n",
      "2013 2014\n",
      "2014 2015\n",
      "2015 2016\n",
      "2016 2017\n",
      "2017 2018\n",
      "2018 2019\n",
      "2019 2020\n",
      "2020 2021\n",
      "2021 2022\n",
      "file_write completed...\n",
      "1 (3, 3)\n",
      "length of df 0\n",
      "2000 2001\n",
      "2001 2002\n",
      "2002 2003\n",
      "2003 2004\n",
      "2004 2005\n",
      "2005 2006\n",
      "2006 2007\n",
      "2007 2008\n",
      "2008 2009\n",
      "2009 2010\n",
      "2010 2011\n",
      "2011 2012\n",
      "2012 2013\n",
      "2013 2014\n",
      "2014 2015\n",
      "2015 2016\n",
      "2016 2017\n",
      "2017 2018\n",
      "2018 2019\n",
      "2019 2020\n",
      "2020 2021\n",
      "2021 2022\n",
      "file_write completed...\n",
      "year range ['2000-2002', '2002-2004', '2004-2006', '2006-2008', '2008-2010', '2010-2012', '2012-2014', '2014-2016', '2016-2018', '2018-2020', '2020-2022']\n",
      "2 (1, 1)\n",
      "length of df 0\n",
      "2000 2002\n",
      "2002 2004\n",
      "2004 2006\n",
      "2006 2008\n",
      "2008 2010\n",
      "2010 2012\n",
      "2012 2014\n",
      "2014 2016\n",
      "2016 2018\n",
      "2018 2020\n",
      "2020 2022\n",
      "file_write completed...\n",
      "2 (2, 2)\n",
      "length of df 0\n",
      "2000 2002\n",
      "2002 2004\n",
      "2004 2006\n",
      "2006 2008\n",
      "2008 2010\n",
      "2010 2012\n",
      "2012 2014\n",
      "2014 2016\n",
      "2016 2018\n",
      "2018 2020\n",
      "2020 2022\n",
      "file_write completed...\n",
      "2 (3, 3)\n",
      "length of df 0\n",
      "2000 2002\n",
      "2002 2004\n",
      "2004 2006\n",
      "2006 2008\n",
      "2008 2010\n",
      "2010 2012\n",
      "2012 2014\n",
      "2014 2016\n",
      "2016 2018\n",
      "2018 2020\n",
      "2020 2022\n",
      "file_write completed...\n",
      "year range ['2000-2003', '2003-2006', '2006-2009', '2009-2012', '2012-2015', '2015-2018', '2018-2021', '2021-2024']\n",
      "3 (1, 1)\n",
      "length of df 0\n",
      "2000 2003\n",
      "2003 2006\n",
      "2006 2009\n",
      "2009 2012\n",
      "2012 2015\n",
      "2015 2018\n",
      "2018 2021\n",
      "2021 2024\n",
      "file_write completed...\n",
      "3 (2, 2)\n",
      "length of df 0\n",
      "2000 2003\n",
      "2003 2006\n",
      "2006 2009\n",
      "2009 2012\n",
      "2012 2015\n",
      "2015 2018\n",
      "2018 2021\n",
      "2021 2024\n",
      "file_write completed...\n",
      "3 (3, 3)\n",
      "length of df 0\n",
      "2000 2003\n",
      "2003 2006\n",
      "2006 2009\n",
      "2009 2012\n",
      "2012 2015\n",
      "2015 2018\n",
      "2018 2021\n",
      "2021 2024\n",
      "file_write completed...\n",
      "year range ['2000-2005', '2005-2010', '2010-2015', '2015-2022']\n",
      "5 (1, 1)\n",
      "length of df 0\n",
      "2000 2005\n",
      "2005 2010\n",
      "2010 2015\n",
      "2015 2022\n",
      "file_write completed...\n",
      "5 (2, 2)\n",
      "length of df 0\n",
      "2000 2005\n",
      "2005 2010\n",
      "2010 2015\n",
      "2015 2022\n",
      "file_write completed...\n",
      "5 (3, 3)\n",
      "length of df 0\n",
      "2000 2005\n",
      "2005 2010\n",
      "2010 2015\n",
      "2015 2022\n",
      "file_write completed...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bigram_num = 50\n",
    "year_gaps = [1,2,3,5]\n",
    "ngrams = {'uni':(1,1),'bigram':(2,2),'trigram':(3,3)}\n",
    "\n",
    "for year_gap in year_gaps:\n",
    "    names = []\n",
    "    year = start_year\n",
    "    while( year<end_year):\n",
    "        first = year\n",
    "        last = year+year_gap\n",
    "        if last==2020 and year_gap==5:\n",
    "            last=2022\n",
    "        str_ = str(first)+'-'+str(last)\n",
    "        names.append(str_)\n",
    "        year = last\n",
    "    print(\"year range\",names)\n",
    "    \n",
    "    \n",
    "    for txt,ngramR in ngrams.items():\n",
    "        \n",
    "        print(year_gap,ngramR)\n",
    "        df_list = []\n",
    "        print(\"length of df\",len(df_list))\n",
    "        year = start_year\n",
    "        for yr_str in names:\n",
    "            first,last = yr_str.split('-')\n",
    "        \n",
    "#         while(year<end_year):\n",
    "#             first = year\n",
    "#             last = year+year_gap\n",
    "#             if last==2020 and year_gap==5:\n",
    "#                 last=2021\n",
    "            working_df = final_df[(final_df['Year']>=int(first))&(final_df['Year']<int(last))]\n",
    "            working_df['text'] =''\n",
    "            for col in working_cols:\n",
    "                working_df['text'] = working_df['text'] + working_df[col]\n",
    "            working_df['text'] = working_df['text'].apply(lambda x:reg_remove_puntuation(x))\n",
    "            working_df['text'] = working_df['text'].apply(lambda x:basic_clean(x))\n",
    "            docs=working_df['text'].tolist()\n",
    "            docs = [i for i in docs if i!=' ']\n",
    "            cv = CountVectorizer(ngram_range=ngramR,stop_words=list(my_stop_words),max_features=2000)#,tokenizer=my_tokenizer,preprocessor=basic_clean)\n",
    "            count_vector=cv.fit_transform(docs)\n",
    "        #     word_set = basic_clean(''.join(str(working_df['text'].tolist())))\n",
    "        #     print('initial_wordset',word_set[:20])\n",
    "            feature_array = np.array(cv.get_feature_names())\n",
    "            cv_sorting = np.argsort(count_vector.toarray()).flatten()[::-1]\n",
    "            print(first,last)\n",
    "            n = bigram_num\n",
    "            top_n = feature_array[cv_sorting][:n]\n",
    "#             print(top_n)\n",
    "            x = {k: v for k, v in cv.vocabulary_.items() if k in top_n}\n",
    "            xdf =pd.DataFrame(x.items())\n",
    "            xdf.sort_values(by=[1],ascending=False)\n",
    "            df_list.append(xdf)\n",
    "            year = last\n",
    "        writer=pd.ExcelWriter(r\"../Data/Paper_Details/Results/\"+txt+\"_CountVec_gap_\"+str(year_gap)+\".xlsx\")\n",
    "        _ = [A.to_excel(writer,sheet_name=\"{0}\".format(names[i])) for i, A in enumerate(df_list)]\n",
    "        print(\"file_write completed...\")\n",
    "        writer.save()\n",
    "        #     print(first,last)\n",
    "        #     bigram_size = (pd.Series(nltk.ngrams(word_set, 3)).value_counts())[:bigram_num]\n",
    "        #     df_list.append(pd.DataFrame(bigram_size))\n",
    "        #     bigram_size.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "        #     year = last\n",
    "        #     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51c36dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(xdf.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cde1b8",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "822758a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"\",\"\",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "40f3e632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year range ['2000-2001', '2001-2002', '2002-2003', '2003-2004', '2004-2005', '2005-2006', '2006-2007', '2007-2008', '2008-2009', '2009-2010', '2010-2011', '2011-2012', '2012-2013', '2013-2014', '2014-2015', '2015-2016', '2016-2017', '2017-2018', '2018-2019', '2019-2020', '2020-2021', '2021-2022']\n",
      "1 (1, 1) []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11114/2420360605.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  working_df['text'] =''\n",
      "/tmp/ipykernel_11114/2420360605.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  working_df['text'] = working_df['text'] + working_df[col]\n",
      "/tmp/ipykernel_11114/2420360605.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  working_df['text'] = working_df['text'].apply(lambda x:reg_remove_puntuation(x))\n",
      "/tmp/ipykernel_11114/2420360605.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  working_df['text'] = working_df['text'].apply(lambda x:basic_clean(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2001\n",
      "2001 2002\n",
      "2002 2003\n",
      "2003 2004\n",
      "2004 2005\n",
      "2005 2006\n",
      "2006 2007\n",
      "2007 2008\n",
      "2008 2009\n",
      "2009 2010\n",
      "2010 2011\n",
      "2011 2012\n",
      "2012 2013\n",
      "2013 2014\n",
      "2014 2015\n",
      "2015 2016\n",
      "2016 2017\n",
      "2017 2018\n",
      "2018 2019\n",
      "2019 2020\n",
      "2020 2021\n",
      "2021 2022\n",
      "file write completed\n",
      "1 (2, 2) []\n",
      "2000 2001\n",
      "2001 2002\n",
      "2002 2003\n",
      "2003 2004\n",
      "2004 2005\n",
      "2005 2006\n",
      "2006 2007\n",
      "2007 2008\n",
      "2008 2009\n",
      "2009 2010\n",
      "2010 2011\n",
      "2011 2012\n",
      "2012 2013\n",
      "2013 2014\n",
      "2014 2015\n",
      "2015 2016\n",
      "2016 2017\n",
      "2017 2018\n",
      "2018 2019\n",
      "2019 2020\n",
      "2020 2021\n",
      "2021 2022\n",
      "file write completed\n",
      "1 (3, 3) []\n",
      "2000 2001\n",
      "2001 2002\n",
      "2002 2003\n",
      "2003 2004\n",
      "2004 2005\n",
      "2005 2006\n",
      "2006 2007\n",
      "2007 2008\n",
      "2008 2009\n",
      "2009 2010\n",
      "2010 2011\n",
      "2011 2012\n",
      "2012 2013\n",
      "2013 2014\n",
      "2014 2015\n",
      "2015 2016\n",
      "2016 2017\n",
      "2017 2018\n",
      "2018 2019\n",
      "2019 2020\n",
      "2020 2021\n",
      "2021 2022\n",
      "file write completed\n",
      "year range ['2000-2002', '2002-2004', '2004-2006', '2006-2008', '2008-2010', '2010-2012', '2012-2014', '2014-2016', '2016-2018', '2018-2020', '2020-2022']\n",
      "2 (1, 1) []\n",
      "2000 2002\n",
      "2002 2004\n",
      "2004 2006\n",
      "2006 2008\n",
      "2008 2010\n",
      "2010 2012\n",
      "2012 2014\n",
      "2014 2016\n",
      "2016 2018\n",
      "2018 2020\n",
      "2020 2022\n",
      "file write completed\n",
      "2 (2, 2) []\n",
      "2000 2002\n",
      "2002 2004\n",
      "2004 2006\n",
      "2006 2008\n",
      "2008 2010\n",
      "2010 2012\n",
      "2012 2014\n",
      "2014 2016\n",
      "2016 2018\n",
      "2018 2020\n",
      "2020 2022\n",
      "file write completed\n",
      "2 (3, 3) []\n",
      "2000 2002\n",
      "2002 2004\n",
      "2004 2006\n",
      "2006 2008\n",
      "2008 2010\n",
      "2010 2012\n",
      "2012 2014\n",
      "2014 2016\n",
      "2016 2018\n",
      "2018 2020\n",
      "2020 2022\n",
      "file write completed\n",
      "year range ['2000-2003', '2003-2006', '2006-2009', '2009-2012', '2012-2015', '2015-2018', '2018-2021', '2021-2024']\n",
      "3 (1, 1) []\n",
      "2000 2003\n",
      "2003 2006\n",
      "2006 2009\n",
      "2009 2012\n",
      "2012 2015\n",
      "2015 2018\n",
      "2018 2021\n",
      "2021 2024\n",
      "file write completed\n",
      "3 (2, 2) []\n",
      "2000 2003\n",
      "2003 2006\n",
      "2006 2009\n",
      "2009 2012\n",
      "2012 2015\n",
      "2015 2018\n",
      "2018 2021\n",
      "2021 2024\n",
      "file write completed\n",
      "3 (3, 3) []\n",
      "2000 2003\n",
      "2003 2006\n",
      "2006 2009\n",
      "2009 2012\n",
      "2012 2015\n",
      "2015 2018\n",
      "2018 2021\n",
      "2021 2024\n",
      "file write completed\n",
      "year range ['2000-2005', '2005-2010', '2010-2015', '2015-2022']\n",
      "5 (1, 1) []\n",
      "2000 2005\n",
      "2005 2010\n",
      "2010 2015\n",
      "2015 2022\n",
      "file write completed\n",
      "5 (2, 2) []\n",
      "2000 2005\n",
      "2005 2010\n",
      "2010 2015\n",
      "2015 2022\n",
      "file write completed\n",
      "5 (3, 3) []\n",
      "2000 2005\n",
      "2005 2010\n",
      "2010 2015\n",
      "2015 2022\n",
      "file write completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bigram_num = 50\n",
    "year_gaps = [1,2,3,5]\n",
    "ngrams = {'uni':(1,1),'bigram':(2,2),'trigram':(3,3)}\n",
    "\n",
    "for year_gap in year_gaps:\n",
    "    names = []\n",
    "    year = start_year\n",
    "    while( year<end_year):\n",
    "        first = year\n",
    "        last = year+year_gap\n",
    "        if last==2020 and year_gap==5:\n",
    "            last=2022\n",
    "        str_ = str(first)+'-'+str(last)\n",
    "        names.append(str_)\n",
    "        year = last\n",
    "        \n",
    "    print(\"year range\",names)\n",
    "    for txt,ngramR in ngrams.items():\n",
    "        df_tf_list = []\n",
    "        print(year_gap,ngramR,df_tf_list)\n",
    "        for yr_str in names:\n",
    "            first,last = yr_str.split('-')\n",
    "            working_df = final_df[(final_df['Year']>=int(first))&(final_df['Year']<int(last))]\n",
    "            working_df['text'] =''\n",
    "            for col in working_cols:\n",
    "                working_df['text'] = working_df['text'] + working_df[col]\n",
    "            working_df['text'] = working_df['text'].apply(lambda x:reg_remove_puntuation(x))\n",
    "            working_df['text'] = working_df['text'].apply(lambda x:basic_clean(x))\n",
    "            docs=working_df['text'].tolist()\n",
    "            tfidf_vectorizer=TfidfVectorizer(use_idf=True,smooth_idf=True,ngram_range=ngramR,max_features=2000,stop_words=my_stop_words) \n",
    "            tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)\n",
    "            feature_array = np.array(tfidf_vectorizer.get_feature_names())\n",
    "            tfidf_sorting = np.argsort(tfidf_vectorizer_vectors.toarray()).flatten()[::-1]\n",
    "            print(first,last)\n",
    "            n = bigram_num\n",
    "            top_n = feature_array[tfidf_sorting][:n]\n",
    "#             df_tf_list.append(pd.DataFrame(top_n))\n",
    "#             print(\"Top N Keywords\\n\\n\")\n",
    "#             print(top_n)\n",
    "            year = last\n",
    "            x = {k: v for k, v in tfidf_vectorizer.vocabulary_.items() if k in top_n}\n",
    "            xdf =pd.DataFrame(x.items())\n",
    "#             print(xdf)\n",
    "            xdf.sort_values(by=[1],ascending=False)\n",
    "            df_tf_list.append(xdf)\n",
    "        writer=pd.ExcelWriter(r\"../Data/Paper_Details/Results/\"+txt+\"_TFiDF_gap_\"+str(year_gap)+\".xlsx\")\n",
    "        _ = [A.to_excel(writer,sheet_name=\"{0}\".format(names[i])) for i, A in enumerate(df_tf_list)]\n",
    "        writer.save()\n",
    "        print(\"file write completed\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc764a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer=pd.ExcelWriter(r\"../Data/Paper_Details/Results/Trigrams_TFIDF.xlsx\")\n",
    "# _ = [A.to_excel(writer,sheet_name=\"{0}\".format(names[i])) for i, A in enumerate(df_tf_list)]\n",
    "# writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0720f3b7",
   "metadata": {},
   "source": [
    "## Streamlit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0dab2f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "69a21042",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip freeze > ../Scripts/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817a9041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
